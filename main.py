"""
main.py
Chatbot WhatsApp - UNA Puno
Versi√≥n unificada: mantiene el estilo y prompts del primer c√≥digo (tono, estructura y reglas)
pero incorpora las mejoras de producci√≥n del segundo (concurrencia, cach√©, locks, timeouts, logs).

Archivo listo para desplegar en Coolify (GitHub).
"""

import os
import json
import logging
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import requests
from dotenv import load_dotenv
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import threading
from functools import lru_cache

# Cliente local para la API de WhatsApp (interfaz m√≠nima esperada)
from whatsapp_client import WhatsAppAPIClient, extract_phone_number

# ---------------------------
# Config & Logging
# ---------------------------
load_dotenv()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s'
)
logger = logging.getLogger(__name__)

# ---------------------------
# Environment / Defaults
# ---------------------------
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'deepseek-v3.1:671b-cloud')  # mantener modelo principal tipo deepseek por defecto
OLLAMA_MODEL_BACKUP = os.getenv('OLLAMA_MODEL_BACKUP', 'gemma3:1b')
OLLAMA_TIMEOUT = int(os.getenv('OLLAMA_TIMEOUT', '30'))

WHATSAPP_API_URL = os.getenv('WHATSAPP_API_URL', 'http://localhost:3000')
WHATSAPP_API_KEY = os.getenv('WHATSAPP_API_KEY', '@12345lin')

MAX_WORKERS = int(os.getenv('MAX_WORKERS', '10'))
POLLING_INTERVAL = int(os.getenv('POLLING_INTERVAL', '3'))
MAX_HISTORY = int(os.getenv('MAX_HISTORY', '5'))

# ---------------------------
# Shared state
# ---------------------------
user_conversations = defaultdict(list)        # phone -> list of exchanges
user_locks = defaultdict(threading.Lock)      # phone -> lock

embedding_model = None
faiss_index = None
documents = []

executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)

# WhatsApp client placeholder (instanciado en main)
whatsapp_client = None

# ---------------------------
# Term expansion (usar versi√≥n extendida del primer c√≥digo)
# ---------------------------
TERM_EXPANSION = {
    'email': ['correo', 'mail', 'correo electr√≥nico', 'correo electronico'],
    'correo': ['email', 'mail', 'correo electr√≥nico', 'correo electronico'],
    'mail': ['email', 'correo', 'correo electr√≥nico'],
    'tel√©fono': ['celular', 'telefono', 'n√∫mero', 'contacto', 'fono'],
    'celular': ['tel√©fono', 'telefono', 'n√∫mero', 'contacto', 'fono'],
    'telefono': ['celular', 'tel√©fono', 'n√∫mero', 'contacto', 'fono'],
    'n√∫mero': ['tel√©fono', 'telefono', 'celular', 'contacto'],
    'contacto': ['tel√©fono', 'telefono', 'celular', 'email', 'correo'],
    'horario': ['hora', 'horarios', 'atenci√≥n', 'atencion'],
    'ubicaci√≥n': ['ubicacion', 'lugar', 'donde', 'direcci√≥n', 'direccion', 'oficina'],
    'estad√≠stica': ['estadistica', 'estad√≠sticas', 'estadisticas', 'FIEI', 'inform√°tica', 'informatica'],
    'inform√°tica': ['informatica', 'estad√≠stica', 'estadistica', 'FIEI', 'sistemas'],
    'ingenier√≠a': ['ingenieria', 'facultad', 'escuela'],
    'agrarias': ['agronom√≠a', 'agronomia', 'agronom√≥nica', 'agronomica', 'FCA'],
    'veterinaria': ['zootecnia', 'FMVZ'],
    'econ√≥mica': ['economica', 'econom√≠a', 'economia', 'FIE'],
    'contables': ['contabilidad', 'administrativas', 'administraci√≥n', 'FCCA'],
    'civil': ['arquitectura', 'FICA'],
    'minas': ['miner√≠a', 'mineria', 'FIM'],
    'qu√≠mica': ['quimica', 'FIQ'],
    'medicina': ['salud', 'FMH'],
}

# ---------------------------
# Knowledge base loading
# ---------------------------

def load_knowledge_base(index_path='knowledge_base.index', json_path='knowledge_base.json'):
    """Cargar embeddings, √≠ndice FAISS y documentos JSON."""
    global embedding_model, faiss_index, documents
    try:
        logger.info("Cargando modelo de embeddings...")
        embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        logger.info("Cargando √≠ndice FAISS...")
        faiss_index = faiss.read_index(index_path)

        logger.info("Cargando documentos...")
        with open(json_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)

        logger.info(f"‚úì Base de conocimiento cargada: {len(documents)} documentos")
        return True
    except Exception as e:
        logger.error(f"Error cargando base de conocimiento: {e}")
        return False


# ---------------------------
# Query expansion & search (con cach√©)
# ---------------------------

def expand_query(query):
    expanded_terms = [query.lower()]
    for word in query.lower().split():
        if word in TERM_EXPANSION:
            expanded_terms.extend(TERM_EXPANSION[word])
    return ' '.join(expanded_terms)


@lru_cache(maxsize=200)
def search_knowledge_base_cached(query, top_k=5):
    return search_knowledge_base(query, top_k, threshold=9.0)


def search_knowledge_base(query, top_k=5, threshold=9.0):
    """Realiza b√∫squeda sem√°ntica con FAISS y filtra por umbral de distancia."""
    if not embedding_model or not faiss_index:
        logger.warning("Base de conocimiento no cargada")
        return []

    try:
        expanded_query = expand_query(query)
        logger.info(f"Query expandida: {expanded_query}")

        query_vector = embedding_model.encode([expanded_query])
        query_vector = np.array(query_vector).astype('float32')

        distances, indices = faiss_index.search(query_vector, top_k)

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(documents):
                doc = documents[idx].copy()
                doc['distance'] = float(dist)
                if len(results) < 3 or dist < threshold:
                    results.append(doc)
        logger.info(f"Resultados filtrados: {len(results)}/{top_k}")
        return results
    except Exception as e:
        logger.error(f"Error en b√∫squeda: {e}")
        return []


# ---------------------------
# Ollama: llamada al modelo
# ---------------------------

def call_ollama(prompt, model_name, stream=True, timeout=OLLAMA_TIMEOUT):
    try:
        url = f"{OLLAMA_URL}/api/generate"
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": 0.7,
                "num_predict": 450,
                "top_p": 0.85,
                "top_k": 5,
                "num_ctx": 3072,
                "repeat_penalty": 1.2
            }
        }

        response = requests.post(url, json=payload, stream=stream, timeout=timeout)
        response.raise_for_status()

        if stream:
            full_response = ""
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                    except Exception:
                        continue
                    if 'response' in data:
                        full_response += data['response']
                    if data.get('done', False):
                        break
            return full_response.strip()
        else:
            return response.json().get('response', '').strip()

    except requests.exceptions.Timeout:
        logger.error(f"Timeout llamando a {model_name} ({timeout}s)")
        return None
    except Exception as e:
        logger.error(f"Error llamando a {model_name}: {e}")
        return None


# ---------------------------
# Prompt y generaci√≥n de respuesta (mantener estilo del primer c√≥digo)
# ---------------------------

def generate_response_dual(user_message, context="", history=""):
    """Sistema dual: intenta con modelo principal y luego con respaldo."""
    system_prompt = r'''Eres el asistente virtual del Vicerrectorado de Investigaci√≥n de la Universidad Nacional del Altiplano (UNA Puno). Combinas profesionalismo con calidez humana.

TU PROP√ìSITO:
Ayudar con informaci√≥n del Vicerrectorado de Investigaci√≥n:
üìß Contactos (emails y tel√©fonos)
üïê Horarios de atenci√≥n  
üìç Ubicaciones
üìö Coordinaci√≥n de investigaci√≥n y tesis
informmacion sobre resoluciones directorales

PERSONALIDAD Y ESTILO:
- Profesional pero cercano - como un buen funcionario universitario
- Usas emojis estrat√©gicamente para dar calidez (2-3 por mensaje)
- Eres claro, directo y √∫til
- Muestras empat√≠a y disposici√≥n a ayudar
- Ofreces informaci√≥n completa pero concisa

TU FORMA DE COMUNICAR:

‚úÖ BIEN:
- "Con gusto üòä, el correo de..."
- "¬°Claro! üìß Te comparto la informaci√≥n..."
- "Por supuesto, aqu√≠ est√° üìù..."
- "Entiendo que necesitas... aqu√≠ est√° üëç"
- "Perfecto, d√©jame ayudarte con eso üìã"

‚ùå EVITA:
- "Seg√∫n consta en el documento..."
- "La base de datos indica..."
- Respuestas de una sola l√≠nea sin contexto
- Ser demasiado formal o fr√≠o

MANEJO DE CONSULTAS FUERA DE ALCANCE:
Si preguntan sobre clima, hora, chistes, noticias, matem√°ticas:
"Disculpa üòä, mi especialidad es la informaci√≥n del Vicerrectorado de Investigaci√≥n. ¬øPuedo ayudarte con alg√∫n contacto de facultad, horario o ubicaci√≥n? üìö"

ESTRUCTURA DE RESPUESTA IDEAL:
1. Saludo breve o confirmaci√≥n amable
2. Informaci√≥n solicitada (espec√≠fica y completa)
3. Ofrecimiento de ayuda adicional

EJEMPLOS MODELO:

Usuario: "correo de ingenier√≠a civil"
T√∫: "¬°Por supuesto! üìß El correo de la Facultad de Ingenier√≠a Civil y Arquitectura es hyanarico@unap.edu.pe. Tambi√©n te comparto que atienden de 8:00 a 14:30 h. ¬øNecesitas el tel√©fono o la ubicaci√≥n? üòä"

Usuario: "tel√©fono de veterinaria"  
T√∫: "Con gusto üìû El tel√©fono de Medicina Veterinaria y Zootecnia es 951644391. Atienden de lunes a viernes de 8:00 a 13:00 h. ¬øTe ayudo con algo m√°s?"

Usuario: "donde esta agrarias"
T√∫: "¬°Claro! üìç La Facultad de Ciencias Agrarias est√° en el pabell√≥n antiguo de Ing. Agron√≥mica, segundo piso, oficina 206, Ciudad Universitaria. Si necesitas contactarlos: üìß fca.ui@unap.edu.pe o üìû 962382228 üòä"

REGLAS FUNDAMENTALES:
- M√°ximo 120 palabras por respuesta
- Usa informaci√≥n del contexto proporcionado directamente
- NO inventes datos que no est√©n en el contexto
- NO mezcles informaci√≥n de diferentes facultades
- Siempre cierra ofreciendo m√°s ayuda
- S√© espec√≠fico: menciona n√∫meros, ubicaciones completas, horarios exactos'''
# Preparar la parte del historial primero (fuera del f-string)
    history_section = f"CONVERSACI√ìN PREVIA:\n{history}\n\n" if history else ""

    if context:
        full_prompt = f"{system_prompt}\n\nINFORMACI√ìN DISPONIBLE:\n{context}\n\n{history_section}PREGUNTA DEL USUARIO: {user_message}\n\nRESPUESTA (m√°ximo 150 palabras):"
    else:
        full_prompt = f"{system_prompt}\n\n{history_section}PREGUNTA DEL USUARIO: {user_message}\n\nRESPUESTA (m√°ximo 150 palabras):"

    logger.info(f"Intentando con {OLLAMA_MODEL}...")
    response = call_ollama(full_prompt, OLLAMA_MODEL, stream=True, timeout=OLLAMA_TIMEOUT)

    if response:
        logger.info(f"‚úì Respuesta de {OLLAMA_MODEL}")
        return response, OLLAMA_MODEL

    logger.warning(f"{OLLAMA_MODEL} fall√≥, usando {OLLAMA_MODEL_BACKUP}...")
    response = call_ollama(full_prompt, OLLAMA_MODEL_BACKUP, stream=True, timeout=max(5, OLLAMA_TIMEOUT // 2))

    if response:
        logger.info(f"‚úì Respuesta de {OLLAMA_MODEL_BACKUP}")
        return response, OLLAMA_MODEL_BACKUP

    return "Lo siento, tengo problemas t√©cnicos. Por favor, intenta de nuevo.", "error"


# ---------------------------
# Conversation history helpers (thread-safe)
# ---------------------------

def get_conversation_history(phone_number):
    with user_locks[phone_number]:
        history = user_conversations[phone_number]
        if not history:
            return ""

        formatted = []
        for entry in history[-MAX_HISTORY:]:
            formatted.append(f"Usuario: {entry['user']}")
            formatted.append(f"Asistente: {entry['bot']}")

        return "\n".join(formatted)


def add_to_history(phone_number, user_msg, bot_msg):
    with user_locks[phone_number]:
        user_conversations[phone_number].append({
            'user': user_msg,
            'bot': bot_msg,
            'timestamp': datetime.now().isoformat()
        })

        if len(user_conversations[phone_number]) > MAX_HISTORY:
            user_conversations[phone_number] = user_conversations[phone_number][-MAX_HISTORY:]

    save_stats_to_file()


# ---------------------------
# Stats & persistence
# ---------------------------

def save_stats_to_file(path='dashboard_stats.json'):
    try:
        stats = {
            'timestamp': datetime.now().isoformat(),
            'total_users': len(user_conversations),
            'total_messages': sum(len(convs) for convs in user_conversations.values()),
            'kb_documents': len(documents),
            'model_used': OLLAMA_MODEL.split(':')[0] if ':' in OLLAMA_MODEL else OLLAMA_MODEL,
            'conversations': []
        }

        for phone, convs in user_conversations.items():
            for conv in convs:
                stats['conversations'].append({
                    'phone': phone,
                    'timestamp': conv['timestamp'],
                    'user': conv['user'],
                    'bot': conv['bot']
                })

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"Error guardando estad√≠sticas: {e}")


# ---------------------------
# Message processing (combina lo mejor de ambos c√≥digos)
# ---------------------------

def process_message(user_message, phone_number):
    user_message = user_message.strip()

    # Comandos especiales
    if user_message.lower() == '/reset':
        with user_locks[phone_number]:
            user_conversations[phone_number] = []
        return "‚úì Conversaci√≥n reiniciada. ¬øEn qu√© puedo ayudarte?"

    if user_message.lower() in ['/ayuda', '/help']:
        return ("Comandos disponibles:\n"
                "/reset - Reiniciar conversaci√≥n\n"
                "/ayuda - Mostrar esta ayuda\n\n"
                "Puedo ayudarte con informaci√≥n sobre:\n"
                "- Coordinadores de facultades\n"
                "- Correos y tel√©fonos\n"
                "- Horarios de atenci√≥n\n"
                "- Ubicaciones\n\n"
                "¬øQu√© necesitas saber?")

    # Filtro de preguntas triviales
    trivial_keywords = [
        'hora', 'fecha', 'd√≠a', 'clima', 'tiempo', 'chiste', 
        'partido', 'f√∫tbol', 'matem√°tica', 'calcula'
    ]

    user_lower = user_message.lower()
    if any(keyword in user_lower for keyword in trivial_keywords):
        uni_keywords = ['universidad', 'facultad', 'coordinador', 'email', 'correo']
        if not any(uni_word in user_lower for uni_word in uni_keywords):
            return "Lo siento, solo puedo ayudarte con informaci√≥n del Vicerrectorado de Investigaci√≥n."

    # Saludo
    if user_message.lower() in ['hola', 'hi', 'hello', 'buenos d√≠as', 'buenas tardes']:
        return "¬°Hola! Soy el asistente virtual del Vicerrectorado de Investigaci√≥n de la UNA Puno. ¬øEn qu√© puedo ayudarte?"

    # B√∫squeda en base de conocimiento (usar cache cuando aplique)
    logger.info(f"Procesando: '{user_message}' de {phone_number}")
    try:
        relevant_docs = search_knowledge_base_cached(user_message[:200], top_k=10)
    except Exception:
        relevant_docs = []

    context = ""
    if relevant_docs:
        context = "\n\n".join([doc['content'][:1000] for doc in relevant_docs[:3]])
        logger.info(f"Contexto encontrado: {len(context)} caracteres")

    history = get_conversation_history(phone_number)
    response, model_used = generate_response_dual(user_message, context, history)

    # Limitar a 1600 caracteres (l√≠mite WhatsApp)
    if len(response) > 1600:
        response = response[:1597] + "..."

    add_to_history(phone_number, user_message, response)

    logger.info(f"Respuesta enviada (modelo: {model_used}): {len(response)} caracteres")
    return response


# ---------------------------
# WhatsApp handler
# ---------------------------

def handle_incoming_message(message):
    """Callback para procesar mensajes entrantes desde la API. Ejecuta el procesamiento en el thread pool."""
    try:
        phone_number = extract_phone_number(message)
        user_message = message.get('body', '').strip()

        logger.info(f"üì® Mensaje de {phone_number}: {user_message[:80]}")

        # Ejecutar en executor para manejar concurrencia
        def task():
            try:
                bot_response = process_message(user_message, phone_number)
                success = whatsapp_client.send_text(phone_number, bot_response)
                if success:
                    logger.info(f"‚úÖ Respuesta enviada a {phone_number}")
                else:
                    logger.error(f"‚ùå Error enviando respuesta a {phone_number}")
            except Exception as e:
                logger.error(f"Error en task de mensaje: {e}", exc_info=True)

        executor.submit(task)

    except Exception as e:
        logger.error(f"Error manejando mensaje entrante: {e}", exc_info=True)


# ---------------------------
# Main / Startup
# ---------------------------

if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("CHATBOT WHATSAPP - VICERRECTORADO DE INVESTIGACI√ìN UNA PUNO")
    logger.info("Versi√≥n: Unificada (estable + optimizada)")
    logger.info("=" * 60)

    # Cargar base de conocimiento
    if not load_knowledge_base():
        logger.error("¬°ERROR! No se pudo cargar la base de conocimiento")
        logger.error("Ejecuta primero: python ingest.py")
        exit(1)

    # Inicializar estad√≠sticas
    logger.info("Inicializando archivo de estad√≠sticas...")
    save_stats_to_file()

    # Inicializar cliente WhatsApp
    logger.info(f"\nüì± Conectando a API de WhatsApp...\n   URL: {WHATSAPP_API_URL}")
    whatsapp_client = WhatsAppAPIClient(WHATSAPP_API_URL, WHATSAPP_API_KEY)

    if not whatsapp_client.check_connection():
        logger.error("\n‚ùå ERROR: No se puede conectar a la API de WhatsApp")
        logger.error("   Aseg√∫rate de que la API est√© corriendo y que WHATSAPP_API_KEY sea correcta")
        exit(1)

    logger.info("‚úÖ Conectado a API de WhatsApp")
    logger.info(f"\nModelo principal: {OLLAMA_MODEL}")
    logger.info(f"Modelo respaldo: {OLLAMA_MODEL_BACKUP}")
    logger.info(f"Workers: {MAX_WORKERS}")
    logger.info(f"Polling interval: {POLLING_INTERVAL}s")
    logger.info("=" * 60)
    logger.info("\nü§ñ Chatbot iniciado. Esperando mensajes... (Ctrl+C para detener)\n")

    # Iniciar polling
    whatsapp_client.start_polling(handle_incoming_message, interval=POLLING_INTERVAL)
